{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod, ABC\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from icecream import ic\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    clipped_x = np.clip(x, -500, 500)  # Prevent overflow\n",
    "    return 1 / (1 + np.exp(-clipped_x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    clipped_x = np.clip(x, -500, 500)  # Prevent overflow\n",
    "    return clipped_x * (1 - clipped_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# def sigmoid_derivative(x):\n",
    "#     return x * (1 - x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Layer(ABC):\n",
    "    \"\"\"Basic building block of the Neural Network\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._learning_rate = 0.01\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward propagation of x through layer\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, output_error_derivative) -> np.ndarray:\n",
    "        \"\"\"Backward propagation of output_error_derivative through layer\"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self._learning_rate\n",
    "\n",
    "    @learning_rate.setter\n",
    "    def learning_rate(self, learning_rate):\n",
    "        assert learning_rate < 1, f\"Given learning_rate={learning_rate} is larger than 1\"\n",
    "        assert learning_rate > 0, f\"Given learning_rate={learning_rate} is smaller than 0\"\n",
    "        self._learning_rate = learning_rate\n",
    "\n",
    "class FullyConnected(Layer):\n",
    "    def __init__(self, input_size: int, output_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.inputs = x\n",
    "        self.outputs = np.dot(x, self.weights) + self.bias\n",
    "        return sigmoid(self.outputs)\n",
    "\n",
    "    def backward(self, output_error_derivative) -> np.ndarray:\n",
    "        sigmoid_derivative_output = sigmoid_derivative(self.outputs)\n",
    "        error_derivative = output_error_derivative * sigmoid_derivative_output\n",
    "        weight_gradients = np.dot(self.inputs.T.reshape(-1, 1), error_derivative.reshape(1, -1))\n",
    "        self.weights -= self.learning_rate * weight_gradients\n",
    "        self.bias -= self.learning_rate * np.sum(error_derivative, axis=0, keepdims=True)\n",
    "        return np.dot(error_derivative, self.weights.T).reshape(self.inputs.shape)\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.inputs = x\n",
    "        self.outputs = np.tanh(x)\n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self, output_error_derivative) -> np.ndarray:\n",
    "        tanh_derivative_output = 1 - np.tanh(self.inputs)**2\n",
    "        return output_error_derivative * tanh_derivative_output\n",
    "\n",
    "class Loss:\n",
    "    def __init__(self, loss_function: callable, loss_function_derivative: callable) -> None:\n",
    "        self.loss_function = loss_function\n",
    "        self.loss_function_derivative = loss_function_derivative\n",
    "\n",
    "    def loss(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Loss function for a particular x and y\"\"\"\n",
    "        return self.loss_function(x, y)\n",
    "\n",
    "    def loss_derivative(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Loss function derivative for a particular x and y\"\"\"\n",
    "        return self.loss_function_derivative(x, y)\n",
    "\n",
    "class MeanSquaredErrorLoss:\n",
    "    @staticmethod\n",
    "    def loss(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        return np.mean((x - y)**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_derivative(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        return 2 * (x - y) / x.size\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    @staticmethod\n",
    "    def loss(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        epsilon = 1e-15\n",
    "        x_clipped = np.clip(x, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y * np.log(x_clipped) + (1 - y) * np.log(1 - x_clipped))\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_derivative(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        epsilon = 1e-15\n",
    "        x_clipped = np.clip(x, epsilon, 1 - epsilon)\n",
    "        return -(y / x_clipped - (1 - y) / (1 - x_clipped)) / x.size\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, layers: List[Layer], learning_rate: float) -> None:\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = None\n",
    "\n",
    "    def compile(self, loss: Loss) -> None:\n",
    "        \"\"\"Define the loss function and loss function derivative\"\"\"\n",
    "        self.loss = loss\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward propagation of x through all layers\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def fit(self,\n",
    "            x_train: np.ndarray,\n",
    "            y_train: np.ndarray,\n",
    "            epochs: int,\n",
    "            verbose: int = 0,\n",
    "            loss: Loss = MeanSquaredErrorLoss) -> None:\n",
    "        \"\"\"Fit the network to the training data\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.learning_rate = self.learning_rate\n",
    "\n",
    "        self.compile(loss)\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for i in range(len(x_train)):\n",
    "                x = x_train[i]\n",
    "                y = y_train[i]\n",
    "                # Forward propagation\n",
    "                output = self(x)\n",
    "\n",
    "                # Compute loss\n",
    "                total_loss += self.loss.loss(output, y)\n",
    "\n",
    "                # Backward propagation\n",
    "                error_derivative = self.loss.loss_derivative(output, y)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error_derivative = layer.backward(error_derivative)\n",
    "\n",
    "            if verbose and epoch % verbose == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {total_loss / len(x_train)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2470171336770237\n",
      "Epoch 1, Loss: 0.15228227897442678\n",
      "Epoch 2, Loss: 0.113334038628934\n",
      "Epoch 3, Loss: 0.08997283981368895\n",
      "Epoch 4, Loss: 0.08032414666310927\n",
      "Epoch 5, Loss: 0.07320191474199839\n",
      "Epoch 6, Loss: 0.06829085313650542\n",
      "Epoch 7, Loss: 0.06512031876869273\n",
      "Epoch 8, Loss: 0.061627331845212296\n",
      "Epoch 9, Loss: 0.05904390901392411\n",
      "Epoch 10, Loss: 0.05756454920133255\n",
      "Epoch 11, Loss: 0.05603329113245437\n",
      "Epoch 12, Loss: 0.0549676256424551\n",
      "Epoch 13, Loss: 0.054039644483366504\n",
      "Epoch 14, Loss: 0.05286067229232598\n",
      "Epoch 15, Loss: 0.05258125244154933\n",
      "Epoch 16, Loss: 0.05245609366052751\n",
      "Epoch 17, Loss: 0.052368052951380493\n",
      "Epoch 18, Loss: 0.05164031942808724\n",
      "Epoch 19, Loss: 0.05099047317651817\n",
      "Accuracy: 0.8055555555555556\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert target labels to one-hot encoding\n",
    "y_train_one_hot = np.eye(10)[y_train]\n",
    "\n",
    "\n",
    "learning_rate = 0.00001\n",
    "# Build and train the network\n",
    "network = Network([FullyConnected(64, 256), Tanh(), Tanh(), FullyConnected(256, 10)], learning_rate=learning_rate)\n",
    "network.compile(loss=MeanSquaredErrorLoss)\n",
    "network.fit(X_train, y_train_one_hot, epochs=20, verbose=1)\n",
    "\n",
    "# Evaluate the network\n",
    "predictions = np.argmax(network(X_test), axis=1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3807006511789819\n",
      "Epoch 1, Loss: 0.3873839163949159\n",
      "Epoch 2, Loss: 0.38936492112868215\n",
      "Epoch 3, Loss: 0.3859346748924096\n",
      "Epoch 4, Loss: 0.37236478374148935\n",
      "Epoch 5, Loss: 0.36537073706029827\n",
      "Epoch 6, Loss: 0.3427777715574182\n",
      "Epoch 7, Loss: 0.3078890884725175\n",
      "Epoch 8, Loss: 0.2715709639961782\n",
      "Epoch 9, Loss: 0.24053367374991771\n",
      "Epoch 10, Loss: 0.21651958681639366\n",
      "Epoch 11, Loss: 0.20935913754538696\n",
      "Epoch 12, Loss: 0.2028029120026361\n",
      "Epoch 13, Loss: 0.19393066765716782\n",
      "Epoch 14, Loss: 0.18737875489341596\n",
      "Epoch 15, Loss: 0.1821547078042899\n",
      "Epoch 16, Loss: 0.17896430621968115\n",
      "Epoch 17, Loss: 0.17592424915991006\n",
      "Epoch 18, Loss: 0.16953376764118225\n",
      "Epoch 19, Loss: 0.1600407398566393\n",
      "Epoch 20, Loss: 0.16067578911936778\n",
      "Epoch 21, Loss: 0.15917245007076197\n",
      "Epoch 22, Loss: 0.15703544981388204\n",
      "Epoch 23, Loss: 0.15821843706198868\n",
      "Epoch 24, Loss: 0.16429967366726853\n",
      "Epoch 25, Loss: 0.16375070034973702\n",
      "Epoch 26, Loss: 0.16008233453311602\n",
      "Epoch 27, Loss: 0.16000320345736097\n",
      "Epoch 28, Loss: 0.158712660286407\n",
      "Epoch 29, Loss: 0.15585372180064197\n",
      "Accuracy: 0.4527777777777778\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.datasets import load_digits\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from typing import List\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     clipped_x = np.clip(x, -500, 500)  # Prevent overflow\n",
    "#     return 1 / (1 + np.exp(-clipped_x))\n",
    "\n",
    "# def sigmoid_derivative(x):\n",
    "#     clipped_x = np.clip(x, -500, 500)  # Prevent overflow\n",
    "#     return clipped_x * (1 - clipped_x)\n",
    "\n",
    "# class Layer:\n",
    "#     def __init__(self):\n",
    "#         self._learning_rate = 0.00001\n",
    "\n",
    "#     @property\n",
    "#     def learning_rate(self):\n",
    "#         return self._learning_rate\n",
    "\n",
    "#     @learning_rate.setter\n",
    "#     def learning_rate(self, learning_rate):\n",
    "#         assert 0 < learning_rate < 1, f\"Invalid learning rate: {learning_rate}\"\n",
    "#         self._learning_rate = learning_rate\n",
    "\n",
    "# class FullyConnected(Layer):\n",
    "#     def __init__(self, input_size: int, output_size: int):\n",
    "#         super().__init__()\n",
    "#         self.input_size = input_size\n",
    "#         self.output_size = output_size\n",
    "#         self.weights = np.random.randn(input_size, output_size)\n",
    "#         self.bias = np.zeros((1, output_size))\n",
    "#         self.inputs = None\n",
    "#         self.outputs = None\n",
    "\n",
    "#     def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "#         self.inputs = x.reshape(-1, self.input_size)\n",
    "#         self.outputs = np.dot(self.inputs, self.weights) + self.bias\n",
    "#         return sigmoid(self.outputs)\n",
    "\n",
    "#     def backward(self, output_error_derivative) -> np.ndarray:\n",
    "#         sigmoid_derivative_output = sigmoid_derivative(self.outputs)\n",
    "#         error_derivative = output_error_derivative * sigmoid_derivative_output\n",
    "#         weight_gradients = np.dot(self.inputs.T, error_derivative)\n",
    "#         self.weights -= self.learning_rate * weight_gradients\n",
    "#         self.bias -= self.learning_rate * np.sum(error_derivative, axis=0, keepdims=True)\n",
    "#         return np.dot(error_derivative, self.weights.T).reshape(self.inputs.shape)\n",
    "\n",
    "# class Network:\n",
    "#     def __init__(self, layers: List[Layer]) -> None:\n",
    "#         self.layers = layers\n",
    "\n",
    "#     def fit(self, x_train: np.ndarray, y_train: np.ndarray, epochs: int, verbose: int = 0) -> None:\n",
    "#         for epoch in range(epochs):\n",
    "#             total_loss = 0\n",
    "#             for i in range(len(x_train)):\n",
    "#                 x = x_train[i]\n",
    "#                 y = y_train[i]\n",
    "\n",
    "#                 # Forward propagation\n",
    "#                 output = x\n",
    "#                 for layer in self.layers:\n",
    "#                     output = layer.forward(output)\n",
    "\n",
    "#                 # Compute loss\n",
    "#                 total_loss += np.mean((output - y) ** 2)\n",
    "\n",
    "#                 # Backward propagation\n",
    "#                 error_derivative = 2 * (output - y) / len(x_train)\n",
    "#                 for layer in reversed(self.layers):\n",
    "#                     error_derivative, = layer.backward(error_derivative)\n",
    "\n",
    "#             if verbose and epoch % verbose == 0:\n",
    "#                 print(f\"Epoch {epoch}, Loss: {total_loss / len(x_train)}\")\n",
    "\n",
    "#     def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "#         for layer in self.layers:\n",
    "#             x = layer.forward(x)\n",
    "#         return x\n",
    "\n",
    "# # Load and preprocess the data\n",
    "# digits = load_digits()\n",
    "# X, y = digits.data, digits.target\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "# y_train_one_hot = np.eye(10)[y_train]\n",
    "\n",
    "# # Build and train the network\n",
    "# network = Network([FullyConnected(64, 256), FullyConnected(256, 10)])\n",
    "# network.fit(X_train, y_train_one_hot, epochs=30, verbose=1)\n",
    "\n",
    "# # Evaluate the network\n",
    "# predictions = np.argmax(network(X_test), axis=1)\n",
    "# accuracy = np.mean(predictions == y_test)\n",
    "# print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
