{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod, ABC\n",
    "from typing import List\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "    \"\"\"Basic building block of the Neural Network\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._learning_rate = 0.01\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x:np.ndarray)->np.ndarray:\n",
    "        \"\"\"Forward propagation of x through layer\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, output_error_derivative) ->np.ndarray:\n",
    "        \"\"\"Backward propagation of output_error_derivative through layer\"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self._learning_rate\n",
    "\n",
    "    @learning_rate.setter\n",
    "    def learning_rate(self, learning_rate):\n",
    "        assert learning_rate < 1, f\"Given learning_rate={learning_rate} is larger than 1\"\n",
    "        assert learning_rate > 0, f\"Given learning_rate={learning_rate} is smaller than 0\"\n",
    "        self._learning_rate = learning_rate\n",
    "\n",
    "class FullyConnected(Layer):\n",
    "    def __init__(self, input_size:int, output_size:int) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x:np.ndarray)->np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_error_derivative)->np.ndarray:\n",
    "        pass\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x:np.ndarray)->np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_error_derivative)->np.ndarray:\n",
    "        pass\n",
    "\n",
    "class Loss:\n",
    "    def __init__(self, loss_function:callable, loss_function_derivative:callable)->None:\n",
    "        self.loss_function = loss_function\n",
    "        self.loss_function_derivative = loss_function_derivative\n",
    "\n",
    "    def loss(self, x:np.ndarray)->np.ndarray:\n",
    "        \"\"\"Loss function for a particular x\"\"\"\n",
    "        pass\n",
    "\n",
    "    def loss_derivative(self, x:np.ndarray, y:np.ndarray)->np.ndarray:\n",
    "        \"\"\"Loss function derivative for a particular x and y\"\"\"\n",
    "        pass\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, layers:List[Layer], learning_rate:float)->None:\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def compile(self, loss:Loss)->None:\n",
    "        \"\"\"Define the loss function and loss function derivative\"\"\"\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x:np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward propagation of x through all layers\"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self,\n",
    "            x_train:np.ndarray,\n",
    "            y_train:np.ndarray,\n",
    "            epochs:int,\n",
    "            learning_rate:float,\n",
    "            verbose:int=0)->None:\n",
    "        \"\"\"Fit the network to the training data\"\"\"\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
